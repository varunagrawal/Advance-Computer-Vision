## Sequence to Sequence Learning with Neural Networks

#### Ilya Sutskever, Oriol Vinyals, Quoc V. Le.
#### NIPS 2014

#### Summary 

In this paper, the authors show that it is possible to use RNNs on unaligned sequence to sequence mapping problems and achieve good performance on the associated task, which in this case is Machine Translation. The authors use a deep LSTM as the RNN model and use one for encoding the input sequence into a fixed representation vector, which is then used to seed a decoding LSTM to generate the translation. While the performance of this model is not state of the art, the novelty is the demonstration that a RNN model can be used on a task with varying input and output sequence lengths without any sort of assumptions about the problem domain and with a limited vocabulary size. When the LSTM is used to rescore the n-best list from a baseline phrase based SMT system, the performance improved significantly and almost rivaled the previous best result published at the time. This is again from an LSTM model that has significant more capacity to learn and improve.
The key technical contribution other than the LSTM is the reversing of the input sequence in order to ease the optimization problem by establishing short term dependencies between the input and the output. This results in significantly improved performance on the task of machine translation when dealing with longer sentences. The authors provide qualitative results as well as PCA projections of the feature representations to visualize them, and report their results using the BLEU metric.

#### Strengths
1. Use of LSTMs demonstrates the application of Deep Networks on problems with sequence data while requiring almost no assumptions and very little fine tuning.
2. Simple trick to reverse the word order of the input sequence which makes intuitive sense.
3. The LSTM works well with long sentences which was a problematic area in previous work.
4. The model is invariant to active/passive voice but "understands"  word ordering in the sequence. This makes arbitrary language translation i.e. any language X to any language Y, easy to do.
5. Overall, this paper's aim is to show that the neural approach to sequence to sequence problems is plausible, opening a new area of research in the progress.

#### Weaknesses
1. Why don't the authors use the full vocabulary once they demonstrate that the model actually works?
2.The authors used PCA for presenting the visualizations of the feature vectors. Why not use tSNE (especially since Ilya's advisor was the one who invented it)?
3. The use of the LSTM seems arbitrary without sufficient reasoning. It seems like the authors just found a (supposedly, as per the impression from the paper) more powerful hammer and decided to hit this nail with it and see if it worked. 
4. Would have been nice to see a baseline with an Elman RNN to illustrate why the LSTM was the best model for the job. Maybe even more datasets?

#### Reflections

This paper is a straightforward one and although hand wavy in certain places, does a good job conveying the idea that deep learning has a family of sequence models that can perform nearly as well as any state of the art system but without the hassles of hand engineering various aspects. In this regard, the paper achieves its goal. The intuitions are well explained and, as we now see, this would lead to RNNs becoming the dominant model for problems with sequence data owing to the generality of the approach illustrated. 