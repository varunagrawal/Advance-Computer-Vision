## Explaining and Harnessing Adversarial Examples

#### Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy
 
#### Summary

This paper examines the properties of neural networks with respect to example inputs which when perturbed slightly result in the network returning incorrect predictions or adversarial examples of inputs. The authors propose that prior hypotheses on overfitting or extreme non-linearity of networks aren't valid and that a linear functional view of neural networks can explain why networks respond like they do to adversarial examples, why the adversarial examples are plentiful and why they generalize to a large number of models despite each model having varying capacities, architectures and training regimes. With this linear explanation, the authors first present a fast and simple method for adversarial example generation based on the gradients of the cost function, thus allowing for tractable experimental analysis of the properties of networks with respect to adversarial examples and allowing models to be trained to be robust against adversarial examples. Indeed, the authors present an adversarial loss function and demonstrate that adversarial training improves the network in terms of regularization over standard techniques such as dropout. The authors provide good arguments for the properties of models as well as for the generalization of adversarial examples. 
 
#### Strengths

Good set of experiments to illustrate the linear behavior of networks and why the fast gradient sign method is able to generate good adversarial examples that generalize to multiple models and can be used for adversarial training.
Fast and simple methods of adversarial perturbation are demonstrated.
Strong arguments for he hypothesis of deep networks being linear, backed by both well-founded theory and empirical results.
The paper addresses a good set of questions regarding adversarial examples and how to make networks robust against them.
 
#### Weaknesses
I would have liked to see some more results on the arguments for Adversarial training vs L1 weight decay.
The authors argue that adversarial examples generalize since most networks approximate a reference function. This seems counter-intuitive and defeats the purpose of different architectures.
 
#### Reflections
This paper, alongwith the Szegedy et. al. paper, provides for an excellent introduction to the area of adversarial examples for deep networks and network robustness against them. Indeed this area has become one of the primary frontiers for deep learning research and having a solid theoretical understanding of deep networks with respect to adversarial examples (which this paper gives us) allows one to delve deeper into this field and make better design considerations for the specific problem being tackled.