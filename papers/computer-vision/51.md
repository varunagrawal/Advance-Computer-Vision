## Analyzing the Behavior of Visual Question Answering Models

#### Aishwarya Agrawal, Dhruv Batra, Devi Parikh. EMNLP 2016.

#### Summary

This paper proposes to analyze the behavior of the current generation of VQA models (circa 2016) by analyzing their behavior in order to elicit understanding of their failure modes, make meaningful comparisons of strengths and weaknesses, and identify avenues for further research. This paper is motivated by the fact that most methods for VQA seem to cluster around the same accuracy range and from prior work in analyzing models and methods such as Derek Hoiem's paper on analyzing object detectors.

The authors consider 3 models, the best performing models from each of the attention and non-attention based classes of models, and the winner of the VQA Challenge 2016 (the MCB model of Fukui et. al.). The behaviors the authors wish to analyze are:

1. How well does a model generalize to novel QI pairs and novel answers or is it "myopic".
2. How much of the question does the model actually "listen" to, or "does it jump to conclusions"?.
3. Does the model even look at the image? Is it "stubborn"?

The authors propose various methods to analyze these properties and behaviors. The findings are below:

1. The models don't quite generalize to new QI pairs (MCB is the exception). No model generalizes to novel answer instances.
2. The models seem to converge on an answer after looking/listening at just half the question. This has an impact on the model's accuracy. Moreover wh-words have a greater effect on accuracy and pronouns in the question "affect" (not effect) the model less (I'm sorry, my inner Grammar nazi couldn't resist).
3. This analysis revealed a significant amount of dataset bias.

#### Strengths

1. A good retrospective paper to help better define new avenues of research in VQA, reminiscent of Hoiem et. al. 2012.
2. Well defined analysis metrics that one can reuse to evaluate their own proposed models.
3. Interesting insights into model behavior and dataset bias. If I had read this paper in 2016, it would have been much more interesting to see these results, but hindsight has played spoiler.


#### Weaknesses

1. Using only 3 models seems restrictive, especially since within attention based models there are subclasses present.
2. There is no justification given for using the euclidean distance metric for the CNN+LSTM model in the generalization test, and for using cosine distance for the other 2 models. It seems untuitive why this would be an apples to apples comparison.
3. No analysis done based on input content, such as question types or image category types as gleaned from the COCO dataset. It feels like a lot more could be done to analyze the behavior of these models and thus this paper feels a bit incomplete.


#### Reflections

Having solid reflections for this paper is a tall order since the obvious issues have already been tackled (one of which we shall read about before the next class). One interesting thought though would be to have the VQA leaderboard display these analyses at the end of each competition, so that one can have a deeper understanding of the top performing models that goes beyond what is written in their respective papers. Having an analysis of this kind on every model of every VQA challenge can help reveal trends of research into VQA and help with future research directions once we feel the field is starting to saturate.