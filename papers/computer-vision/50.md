## Hierarchical Question-Image Co-Attention for Visual Question Answering

#### Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh. NIPS 2016.

#### Summary

This paper presents the idea of attention on both the image features as well as the question for the task of Visual Question Answering, which the authors term as co-attention. The co-attention is modeled via a 3 level hierarchy, at the question word level, phrase level and the question level. At each level, the model learns to co-attend to the image and the particular n-gram of that level. For the case of the phrase level, the authors introduce 1-d convolutions over unigrams, bigrams and trigrams, then capturing the phrase level semantics via a max pool across the n-grams.

The authors propose two methods for co-attention, the Parallel Co-Attention which generates individual attention maps for the image and question features based on an affinity matrix between them, and the Alternating Co-Attention which alternates between sampling attention maps from the image features and the question features in order to compute attention maps for the other. This is done at each level of the hierarchy, with each method having its pros and cons during experimentation. Finally, the attended image and question features at different levels of the hierarchy are encoded via a recursive scheme in order to predict the final answer.

Finally, the authors compare their proposed technique against state-of-the-art techniques and demonstrate improvements on both the VQA dataset and the COCO-QA dataset. An ablation study validates and quantifies the contributions of each of their design elements, such as co-attention, 1-d convolution + pooling, and 3 layer hierarchy of question representation.

#### Strengths

1. The 3 layer hierarchy for the question representation and co-attention allows the model to capture both high level semantics as well as finer granularity on the word level, thus stronger representation for VQA.
2. The idea of co-attention is intuitive and can be further generalized to different modalities.
3. The 1-d convolution at the phrase level presents an interesting conceptualization of capturing neighboring word relations over the word vector representations.
4. Ablation study to quantify the value of each of the proposed ideas is very informative and makes a cogent argument for this paper.


#### Weaknesses

1. The paper performs the encoding of the co-attention in the hierarchy in a recursive format. This seems prone to causing the network to provide unequal weightage of the different hierarchy levels which is counterintuitive from what one sees in the case of similar granularity capturing techniques such as skip connections. It would be nice to see alternate formulations such as simple concatenation and how that squares up against this recursive encoding scheme.
2. I would have liked to see a better explanation of the design of the equations for the parallel co-attention. Since the affinity matrix is already capturing a representation of Visual and Question features, using each of them again to find the attention weights for both the question and the image seems redundant and prone to overfitting.
3. While using upto trigrams makes sense since prior work demonstrates that upto 3 words of the question are sufficient to model the pragmatics of the question, a study on the choice of n-grams would have been interesting. 


#### Reflections

One interesting thought would be to allow the Alternating Co-Attention to pick from the the raw attention values and the co-attended attention values. That is, when the question co-attention is being computed, give it both the raw visual attention and the question guided visual attention and let it choose (similarly for the image co-attention).
The idea of co-attention is potent and similar to Multimodal Compact Bilinear pooling can be applied to multiple different domains and problems. I would like to see how these various simple techniques (co-attention, MCB pooling, neural modules) work together and whether they are able to outperform the state-of-the-art on VQA. A simple idea, perhaps good for the VQA challenge, but very useful nonetheless.