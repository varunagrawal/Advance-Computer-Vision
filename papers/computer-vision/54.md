## Visual Dialog

#### Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos√© M. F. Moura, Devi Parikh, Dhruv Batra. CVPR 2017.

#### Summary

In this paper, the authors propose the task of Visual Dialog. The idea is to have a chatbot that is capable of having a conversation with a human about an image, thus leading to a natural extension of VQA. The task is different from VQA due to the fact that the chatbot is now required to not only answer questions about the image but also maintain history of the conversation in order to perform co-reference resolution in follow up questions. The authors claim this task serves as a good test of machine intelligence since it is not tied to a particular downstream task and is between the spectrum endpoints of goal driven dialog and normal chit-chat.

To this end, the authors use AMT to collect a dataset of conversations about an image between 2 turkers. They accomplish this via a custom built AMT interface since AMT lacks the capability to host multi-person HITs. The task is setup to allow one turker the view of the image and to keep the image hidden from the second turker. The second turker now has to ask questions (primed with a caption of the image) in order to "imagine the scene better". The authors then provide an analysis of the dataset in order to validate its richness and understand the types and properties of the questions in the dataset.

In the final part of the paper, the authors propose different types of neural encoders and decoders in order to establish baselines for the Visual Dialog task. They provide details about the models and how they handle the unique aspects of Visual Dialog such as maintaining history state. After providing a retrieval based metric to evaluate the models, the authors put forth baselines.

#### Strengths

1. Novel task which is a natural extension to VQA.
2. The new neural encoders are novel in the ability to maintain history of the dialog, particularly the HRE and Memory Network Encoder.
3. A wide variety of analyses on the dataset help to understand the types of questions and what may be required to do well on this task.

#### Weaknesses

1. The authors use single dialogs between 2 AMT workers. This makes the dataset prone to the biases of the workers. It would have been nice to include multiple dialogs per image with different pairings of AMT workers.
2. Large class imbalance in the answers. This is a known problem with VQA 1.0 and the nature of the task given to the turkers seems to have caused a similar bias here. 
3. There seems to be a significant number of answers that go "I can't tell" or "I don't know". It would have been nice to control for these and then use additional methods such as the ones proposed by Ray et. al. to help the model identify false premise or non-visual questions.

#### Reflections

An interesting idea that came to me at the beginning of the fall 2017 semester was to use visual dialog in order to understand not just an image but the environment of the human in order to achieve a shared task. For example, if a blind man is trying to pick up an object, asking questions to this chatbot in natural language would help the man locate and pick up the object and the bot would also be rewarded for its help, leading to shared reward for a common goal. This seems to be the next logical step for visual dialog.
