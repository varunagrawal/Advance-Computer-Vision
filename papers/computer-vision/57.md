## Natural Language Does Not Emerge "Naturally" in Multi-Agent Dialog

#### Satwik Kottur, Jos√© M.F. Moura, Stefan Lee, Dhruv Batra. EMNLP 2017.

#### Summary

In this paper, the authors showcase a sequence of results related to language learning in cooperative mutli agent dialog settings. The overall goal is to understand what are the conditions required that lead to interpretable and compositional grounded language to emerge in multi agent dialog settings. In order to do so, a cooperative reference game, Task and Talk, set up as a test bed. In this game, the Q bot is only given the task of identifying two attributes of an object and cannot see the object in question which has 3 attributes. The A bot can see the object and its attributes perfectly but does not know of the task. This information asymmetry is by design in order to induce dialog to solve the task given successfully. The agents are trained using the REINFORCE algorithm and both are modeled in existing in partially observable stochastic environments. The model details are given and are straightforward, reminiscent of Visual Dialog.

For the first illustration, the vocabulary of both bots are unbounded, leading to the A bot ignoring the Q bots utterances and simply signaling its state to the Q bot over the 2 rounds of dialog. This is akin to the Lewis Signaling game and is thus a collapse of the dialog, leading to the intuition that a more human-like limited vocabulary that is not capable of completely capturing the richness of the world is required to force dialog.

In the second setting, the authors limit the vocabularies of each bot to the minimum number of symbols required to encode the task and attribute values of the Q bot and A bot respectively. This leads to the Q bot only using the first dialog round in order to communicate its task in an order agnostic fashion, as well as the A bot opting to not ground its symbols into individual states, opting to perform a type of set partitioning where it maps each pair of attributes to a pair of symbols from each of its dialog utterances, thus making the second utterance dependent on the first one.

In the final setting, the authors take away the memory capability of the A bot (basically re-initialize the hidden state at each round) and further reduce the vocabulary of the A bot in order to limit the number of synonyms the A bot can learn. The authors notice that this leads to strong compositionality and grounding of the language, correspondingly performing well on the test set.

Finally, the authors illustrate the evolution of the learned language as well as the points where the grounding happens by depicting the dialog as a dialog tree where each leaf corresponds to an attribute pair that accomplishes the task. Observations from the evolution timeline show the strong correlation between grounding and improvement in task performance.

#### Strengths

1. The evidence of a correlation between model performance and language grounding is a key takeaway from this paper proving the importance of grounding language for a task.
2. Dialog tree is a cool way to show the language evolution.
3. This paper is well written and easy to understand while also completing the story that started in the Visual Dialog paper.

#### Weaknesses

1. This paper primarily draws from the observations in the Visual Dialog paper seen during the sanity check and further fleshes it out, thus I would argue that there isn't much novelty other than the thoroughness of the analysis, and the language evolution graphs. 
2. It's surprising to see no mention of training collapse i.e. the authors seem to be able to train these models without any sort of trouble despite the possibilty of these models not training at all at each of the settings. I don't know if this is misleading (though I'm about to find out).
3. It would have been really nice to see results on a similar dataset like CLEVR in order to corroborate these initial results.

#### Reflections

This paper provides a scientific hypothesis model for the emergence of grounded and compositional language derived from key insights from prior experiments. I can see this paper being used to help design future experiments in this space, but not much more empirically. The next logical step here would be for linguists and deep learning theory researchers to come up with a solid theory of this behaviour grounded in semantics of the task, the complexity of the deep learning models and other conditions to provide a unified theory for this behavior.
