## Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning

####Abhishek Das, Satwik Kottur, Jos√© M.F. Moura, Stefan Lee, Dhruv Batra. ICCV 2017.

#### Summary

In this paper, the authors propose the first goal driven training scheme for learning Visual Dialog. They set up the training as a Reinforcement Learning problem with 2 chat-bots, a Q bot and an A bot, learning to communicate with each other in order to do well on a task. The task in question here is for the Q bot to ask the A bot questions about an image that only the A bot can see and try to build a mental model of the image in order to guess the right image from a set of images.

In order to do this, the authors use the Visual Dialog dataset. The Q bot has a fact encoder, a history encoder and a generative question decoder and is trained to ask questions conditioned on the image caption. To guess the image, the Q bot has a feature regression network that tries to predict the correct representation given the state of the Q bot. The A bot has a question encoder, a fact encoder, a history encoder and an answer decoder in order to generate dialog like answers for the Q bot's questions. Both models are trained using the REINFORCE algorithm in order to learn the correct policies.

Two experiments are conducted and the training scheme varies per experiment. For the first experiment, as a sanity check, the bots are given a synthetic dataset of images with attribute labels and asked to complete the task. The result of this experiment was that the 2 bots managed to figure out a common grammar to associate and communicate the required attributes, thus leading to an emergence of grounded dialog. The language was not english like, however.

In the second experiment, the authors follow a curriculum based training regime. They first train the boths on Visual Dialog in a supervised fashion in order to ground the bots in english language rather than have them reinvent english. After that they slowly increase the number of rounds they use the REINFORCE algorithm for training, starting with supervised and then switching to REINFORCE incrementally. The results from the paper show that this approach to model training improves performance of the models on standard Visual Dialog tasks with the hypothesis that this goal driven approach leads to them learning from each other in order to work collaboratively and thus improving overall performance.

#### Strengths

1. Novel goal driven approach to training Visual Dialog models.
2. The emergence of natural language with the synthetic dataset was highly intriguing and could be an avenue for further research.
3. Human studies instill faith in this methodology.

#### Weaknesses

1. The authors imply in the beginning that the Q bot has to choose from a pool of images but the final model directly predicts the feature representation of the image using its own state. Thus there is a disconnect in the paper.
2. While REINFORCE is a fine choice, using perhaps one more policy gradient optimization algorithm in order to do a comparison would have been a nice to have.


#### Reflections

This paper provides a general framework for training models using a reinforcement learning approach. While I can see this framework being adopted more widely due to its simplicity (just look at GANs!), there is still the problem of interpretability and understanding what are the parameters to tune to get RL algorithms to consistently work better than fully supervised or semi-supervised algorithms. These are interesting questions which are currently being tackled by the RL community at least and perhaps with more insight and analysis we can provide similar results, especially with datasets such as VQA 2.0 that allow for explainablity and other techniques like Vedantam et. al.'s Context Aware Captioning.
