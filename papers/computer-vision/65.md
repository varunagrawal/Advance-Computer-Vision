## Adversarial Patch

#### Tom B. Brown, Dandelion Mane, Aurko Roy, Martin Abadi, Justin Gilmer
#### NIPS 2017

#### Summary 

In this paper, the authors demonstrate a technique to create a universal, robust, and targeted adversarial patch. This means that the adversarial patch can be applied to any location in any image and a Deep Network classifier will likely output the targeted class and not the true class.

This work uses large pertubation of a starting patch. This is in contrast with most methods that apply small, imperceptible perturbations to the image. The authors argue that most images that are operated on by Machine Learning models don't go through human validation and thus the need for small, imperceptible perturbations isn't quite required.

Experiments show that their adversarial patches are indeed more effective and even transfer well to the real world, i.e. the patch is printed out, placed in a scene and the scene is classified, giving the targeted output.

#### Strengths

1. Demonstration of a universal adversarial patch is the key concept, as it does not require knowledge of the image or the model.
2. Simple training procedure to generate the patch from the starting patch, which can be replicated without too much trouble.
3. The real-world transferrability is a very useful application of this.

#### Weaknesses

1. Lack of extensive analyses. The authors do not go into metrics over diferent object classes and categories. They discuss that the size of the patch in their method is larger than other methods, but don't discuss what sizes affect by how much.
2. There is a lack of sufficient examples. Only a single example (a toaster) is shown.
