## Deep Compositional Question Answering with Neural Module Networks

#### Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein. CVPR 2016.

#### Summary

This paper tackles deep compositional visual QA by presenting a technique for analyzing a natural language question using a semantic parser and then based on the parse tree dependencies, illustrates how to construct a deep network from a set of predefined neural modules, each module being constructed to tackle a particular task such as generate attention maps, merge attention maps or perform classification. Hence this paper draws from prior work in deep learning and logical reasoning, but while the network layout is grounded in formal logic, the inference is done in the representation space of deep networks.

The authors provide a minimal set of neural modules for their specific task and show how to perform the assembly of the NMN (Neural Module Network) and train it. This is done on a per question basis with each of the modules being trained for that image-question-answer instance, thus each module sees a subset of the data for which it is deemed useful and building up its specialty. To demonstrate their technique, they show empirical results on 2 datasets, the VQA dataset, where their NMN model performs as well as the state of the art, as well as their custom Shapes dataset where the questions are much more complex and compositional. Their technique outperforms other state of the art techniques for VQA by a large margin and validates their design. The authors also discuss avenues for future work as well as applicability to other tasks and domains. 

#### Strengths

1. Novel formulation that "constructs" deep networks on the fly and allows for different modules to learn specific features.
2. Plug and play model for different neural modules, thus allowing extensibility to the wide range of layers and designs in the literature.
3. Shapes dataset provides for more complex compositional VQA task.


#### Weaknesses

1. The design and choices of the neural modules seems arbitrary. There seems to be motivation based on the VQA task at hand but not much elaboration on the specifics of each module.
2. The authors perform a later fusion of the question representation with the output of the NMN, citing common sense reasoning as the motivation. Prior work has utilized question features to help direct attention whereas this work does not, utilizing "question features" only for generating the NMN. This seems counterintuitive especially since the question parser is a separate component and not trained jointly with the NMN. Having the question features from an RNN along with the image features from a CNN go into the NMN together would have been a good experiment to perform in this paper.

As an aside, this paper is overall quite poorly written with grammatical errors in almost every major section, along with one error in the description of the attend module that was particularly glaring. I am quite surprised this paper was accepted without significant revisions into CVPR.

#### Reflections

The idea of reuseable modules that are trained based on specific data subsets which are geared for those modules is very interesting, and what I would have liked to see is the use of this technique on established layers in the literature and how much each of those layers actually contribute to the task at hand.
As a concrete example, for image geolocation, we have the PlaNet architecture but also have additional layers and poolings defined (such as NetVLAD and Multiscale orderless poolinh) which help tackle certain aspects of the problem. Running an experiment with such established modules and quantifying their individual contributions to the overall task would be a big step towards unraveling the black magic that is Deep Network design.